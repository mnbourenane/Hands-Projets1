{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Handwritten Digit Recognition Project**\n",
        "*   Loading the MNIST Dataset"
      ],
      "metadata": {
        "id": "mtKoX8inggNs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4Gp5k76gTro"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# TODO: Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# TODO: Load the MNIST dataset\n",
        "train_set = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_set = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# TODO: Create data loaders\n",
        "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Figure out how many images are in the train_set and test_set.\n",
        "num_train_images = len(train_set)\n",
        "num_test_images = len(test_set)\n",
        "print(f\"the number of images in the train_set is : {num_train_images}\")\n",
        "print(f\"the number of images in the test_set is : {num_test_images}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ne7HKz61hW83",
        "outputId": "6b1834d8-b10c-42fa-c127-dc8c141124b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the number of images in the train_set is : 60000\n",
            "the number of images in the test_set is : 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building the Neural Network Model"
      ],
      "metadata": {
        "id": "8NWZGPS6lrlf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary PyTorch libraries\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# TODO: Define the neural network class\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # TODO: Define layers of the neural network\n",
        "        self.fc1 = nn.Linear(28 * 28, 10) # First fully connected layer\n",
        "        #self.fc2 = nn.Linear(1024, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Flatten the input tensor\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = self.fc1(x)  # TODO: add an activation function\n",
        "        return x\n",
        "\n",
        "# Create an instance of the network\n",
        "model = Net()\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyUkgL2oluJ5",
        "outputId": "f7bebcb7-f6d6-4e3f-e546-6f637e7f8dcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=784, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary PyTorch libraries\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# TODO: Define the neural network class\n",
        "class ComplexNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ComplexNet, self).__init__()\n",
        "        # TODO: Define layers of the neural network\n",
        "        self.fc1 = nn.Linear(28*28, 1024) # First fully connected layer\n",
        "        self.fc2 = nn.Linear(1024, 128)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Flatten the input tensor\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x\n",
        "\n",
        "# Create an instance of the network\n",
        "model = ComplexNet()\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CrMF9TwqzXv",
        "outputId": "c415098d-450a-4a35-cebf-2a193569d227"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ComplexNet(\n",
            "  (fc1): Linear(in_features=784, out_features=1024, bias=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=128, bias=True)\n",
            "  (fc3): Linear(in_features=128, out_features=10, bias=True)\n",
            "  (relu): ReLU()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the Neural Network Model"
      ],
      "metadata": {
        "id": "EzowzdhRoNKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Complete this code\n",
        "# Import optimizer\n",
        "from torch.optim import SGD\n",
        "\n",
        "# TODO: Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# TODO: Set the number of epochs\n",
        "num_epochs = 50\n",
        "hist_training_loss = []\n",
        "hist_test_loss = []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        # TODO: Complete Training pass\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # compute output\n",
        "        # compute loss\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        #execute backword and optimaze\n",
        "        loss.backward() #Compute Grandients\n",
        "        optimizer.step() #Adjust the weights\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        hist_training_loss.append(running_loss)\n",
        "    else:\n",
        "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "    # TODO: evaluate on the test_loader\n",
        "    test_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "     for images, labels in test_loader:\n",
        "        # TODO: Complete evaluation pass\n",
        "            outputs = model(images)\n",
        "            tloss = criterion(outputs, labels)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "\n",
        "     else:\n",
        "         print(f\" Loss: {test_loss/len(test_loader)}\")\n",
        "         hist_test_loss.append(test_loader)\n",
        "\n",
        "print(\"Training is finished!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Tf3r8kloZ2a",
        "outputId": "b95562a7-9afa-4086-d134-5bba22c64cc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.8991490215507906\n",
            " Loss: 0.43052107095718384\n",
            "Epoch 2, Loss: 0.36328197987094873\n",
            " Loss: 0.27053311467170715\n",
            "Epoch 3, Loss: 0.30858045340632834\n",
            " Loss: 0.5897347927093506\n",
            "Epoch 4, Loss: 0.27543183181013886\n",
            " Loss: 0.14713388681411743\n",
            "Epoch 5, Loss: 0.24941754390372398\n",
            " Loss: 0.42356792092323303\n",
            "Epoch 6, Loss: 0.22530470687999274\n",
            " Loss: 0.36445552110671997\n",
            "Epoch 7, Loss: 0.2038497541909978\n",
            " Loss: 0.2702961266040802\n",
            "Epoch 8, Loss: 0.18527632930290217\n",
            " Loss: 0.21242643892765045\n",
            "Epoch 9, Loss: 0.1686818336071109\n",
            " Loss: 0.12029032409191132\n",
            "Epoch 10, Loss: 0.1548410837691444\n",
            " Loss: 0.025899788364768028\n",
            "Epoch 11, Loss: 0.14269546409254708\n",
            " Loss: 0.14279554784297943\n",
            "Epoch 12, Loss: 0.13176265165114454\n",
            " Loss: 0.05459611117839813\n",
            "Epoch 13, Loss: 0.1218642116591795\n",
            " Loss: 0.03464925289154053\n",
            "Epoch 14, Loss: 0.11382034674350387\n",
            " Loss: 0.07225623726844788\n",
            "Epoch 15, Loss: 0.1064557311540124\n",
            " Loss: 0.08335117995738983\n",
            "Epoch 16, Loss: 0.09931161255240123\n",
            " Loss: 0.03361031413078308\n",
            "Epoch 17, Loss: 0.09309602882077636\n",
            " Loss: 0.1355004608631134\n",
            "Epoch 18, Loss: 0.08733682742100089\n",
            " Loss: 0.1753353476524353\n",
            "Epoch 19, Loss: 0.08247184118172571\n",
            " Loss: 0.0724131241440773\n",
            "Epoch 20, Loss: 0.07771118021885882\n",
            " Loss: 0.09698769450187683\n",
            "Epoch 21, Loss: 0.07333589416307046\n",
            " Loss: 0.09867864102125168\n",
            "Epoch 22, Loss: 0.06924870474957057\n",
            " Loss: 0.009398465044796467\n",
            "Epoch 23, Loss: 0.0654848797027387\n",
            " Loss: 0.04508253186941147\n",
            "Epoch 24, Loss: 0.06220158370040945\n",
            " Loss: 0.061419565230607986\n",
            "Epoch 25, Loss: 0.05901593425279376\n",
            " Loss: 0.05726318061351776\n",
            "Epoch 26, Loss: 0.055802092260953146\n",
            " Loss: 0.0515732578933239\n",
            "Epoch 27, Loss: 0.052919295107596305\n",
            " Loss: 0.022556666284799576\n",
            "Epoch 28, Loss: 0.05065913714502237\n",
            " Loss: 0.018876144662499428\n",
            "Epoch 29, Loss: 0.04779952754532652\n",
            " Loss: 0.07050246000289917\n",
            "Epoch 30, Loss: 0.045692762765568745\n",
            " Loss: 0.03309766948223114\n",
            "Epoch 31, Loss: 0.04353743356003213\n",
            " Loss: 0.04950873553752899\n",
            "Epoch 32, Loss: 0.04163296027297674\n",
            " Loss: 0.018134484067559242\n",
            "Epoch 33, Loss: 0.0392877539582074\n",
            " Loss: 0.02789989486336708\n",
            "Epoch 34, Loss: 0.037478641593711635\n",
            " Loss: 0.010782524943351746\n",
            "Epoch 35, Loss: 0.035845375674437165\n",
            " Loss: 0.04033718258142471\n",
            "Epoch 36, Loss: 0.0343031109298772\n",
            " Loss: 0.007153408136218786\n",
            "Epoch 37, Loss: 0.03245571348518292\n",
            " Loss: 0.0033828564919531345\n",
            "Epoch 38, Loss: 0.030921364984741167\n",
            " Loss: 0.017265375703573227\n",
            "Epoch 39, Loss: 0.02943231935860085\n",
            " Loss: 0.040880486369132996\n",
            "Epoch 40, Loss: 0.028275896542902187\n",
            " Loss: 0.055435363203287125\n",
            "Epoch 41, Loss: 0.027014490716886132\n",
            " Loss: 0.004052313510328531\n",
            "Epoch 42, Loss: 0.025859933326155868\n",
            " Loss: 0.15929336845874786\n",
            "Epoch 43, Loss: 0.024528293419993723\n",
            " Loss: 0.022859716787934303\n",
            "Epoch 44, Loss: 0.023503968033687407\n",
            " Loss: 0.1191781759262085\n",
            "Epoch 45, Loss: 0.02233307494241784\n",
            " Loss: 0.04332555830478668\n",
            "Epoch 46, Loss: 0.02139553553444633\n",
            " Loss: 0.018976956605911255\n",
            "Epoch 47, Loss: 0.020462239438768175\n",
            " Loss: 0.05497293919324875\n",
            "Epoch 48, Loss: 0.01943891672546596\n",
            " Loss: 0.006574694067239761\n",
            "Epoch 49, Loss: 0.01861114475850278\n",
            " Loss: 0.0081990547478199\n",
            "Epoch 50, Loss: 0.017816399714337593\n",
            " Loss: 0.08111561089754105\n",
            "Training is finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: plot the model complexity graph\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "epoch_list = np.arange(50)\n",
        "plt.plot(epoch_list, hist_training_loss, label='Training loss')\n",
        "plt.plot(epoch_list, hist_test_loss, label='Test loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.title('Training and Test Losses')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 809
        },
        "id": "VWFfsSpjm7ym",
        "outputId": "4c5d6ff5-b6f5-42d5-ceae-3649deae7ec4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-7d147c41a717>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mepoch_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhist_training_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhist_test_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Test loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epochs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2810\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2811\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2812\u001b[0;31m     return gca().plot(\n\u001b[0m\u001b[1;32m   2813\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2814\u001b[0m         **({\"data\": data} if data is not None else {}), **kwargs)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m         \"\"\"\n\u001b[1;32m   1687\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1689\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m             yield from self._plot_args(\n\u001b[0m\u001b[1;32m    312\u001b[0m                 this, kwargs, ambiguous_fmt_datakey=ambiguous_fmt_datakey)\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    505\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    506\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (50,) and (46900,)"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing Early Stopping 🛑"
      ],
      "metadata": {
        "id": "Jt9x3Wj64aZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Complete this code\n",
        "# Import optimizer\n",
        "from torch.optim import SGD\n",
        "\n",
        "# TODO: Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# TODO: Set the number of epochs\n",
        "num_epochs = 50\n",
        "\n",
        "hist_training_loss = []\n",
        "hist_test_loss = []\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        # TODO: Complete Training pass\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward() # computes gradients\n",
        "        optimizer.step() # adjust the weights\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    else:\n",
        "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
        "    hist_training_loss.append(running_loss)\n",
        "    # TODO: evaluate on the test_loader\n",
        "    test_loss = 0.0\n",
        "    with torch.no_grad(): # don't compute any gradients\n",
        "      for images, labels in test_loader:\n",
        "          # TODO: Complete evaluation pass\n",
        "          outputs = model(images)\n",
        "          tloss = criterion(outputs, labels)\n",
        "\n",
        "          test_loss += tloss.item()\n",
        "\n",
        "      else:\n",
        "          print(f\" Loss: {test_loss/len(test_loader)}\")\n",
        "      hist_test_loss.append(test_loss)\n",
        "\n",
        "print(\"Training is finished!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4F5Tx9Fzyekz",
        "outputId": "572548ce-d5be-4d50-cac5-2c8f4bc966b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.017055021750970064\n",
            " Loss: 0.0638396362569235\n",
            "Epoch 2, Loss: 0.01622227756503864\n",
            " Loss: 0.06626679809238675\n",
            "Epoch 3, Loss: 0.015556501333853964\n",
            " Loss: 0.06317579306917528\n",
            "Epoch 4, Loss: 0.014749840231808916\n",
            " Loss: 0.06601695810254829\n",
            "Epoch 5, Loss: 0.014361883827690174\n",
            " Loss: 0.06419447942941478\n",
            "Epoch 6, Loss: 0.013639181372220577\n",
            " Loss: 0.06453421075548828\n",
            "Epoch 7, Loss: 0.013237565001502617\n",
            " Loss: 0.06350397682130696\n",
            "Epoch 8, Loss: 0.012618503551051489\n",
            " Loss: 0.06763139609582974\n",
            "Epoch 9, Loss: 0.012094458195205623\n",
            " Loss: 0.06453402562824724\n",
            "Epoch 10, Loss: 0.01148361233647601\n",
            " Loss: 0.06533663095114826\n",
            "Epoch 11, Loss: 0.01100310816048289\n",
            " Loss: 0.06497710114119724\n",
            "Epoch 12, Loss: 0.010598380873353332\n",
            " Loss: 0.06573755175880361\n",
            "Epoch 13, Loss: 0.010100149514234519\n",
            " Loss: 0.06962684306060632\n",
            "Epoch 14, Loss: 0.00983679877339191\n",
            " Loss: 0.06418518693171327\n",
            "Epoch 15, Loss: 0.009384668360316336\n",
            " Loss: 0.06752367016071881\n",
            "Epoch 16, Loss: 0.009034954617718366\n",
            " Loss: 0.0649965001751176\n",
            "Epoch 17, Loss: 0.008681514454116402\n",
            " Loss: 0.06394726193828285\n",
            "Epoch 18, Loss: 0.008343629266094686\n",
            " Loss: 0.06591721965304233\n",
            "Epoch 19, Loss: 0.008008941400623413\n",
            " Loss: 0.06723608111392741\n",
            "Epoch 20, Loss: 0.007800851690875583\n",
            " Loss: 0.06687242258421222\n",
            "Epoch 21, Loss: 0.007369949687969809\n",
            " Loss: 0.06536069696203355\n",
            "Epoch 22, Loss: 0.007204306903941833\n",
            " Loss: 0.0785377825231063\n",
            "Epoch 23, Loss: 0.006914283838003937\n",
            " Loss: 0.06559723280041659\n",
            "Epoch 24, Loss: 0.006751943068651568\n",
            " Loss: 0.06635159019347997\n",
            "Epoch 25, Loss: 0.006391166160982758\n",
            " Loss: 0.0661424050043366\n",
            "Epoch 26, Loss: 0.006265022246116675\n",
            " Loss: 0.06627686593958153\n",
            "Epoch 27, Loss: 0.005992709620751993\n",
            " Loss: 0.06688882341698961\n",
            "Epoch 28, Loss: 0.005804617930647246\n",
            " Loss: 0.06569827659080431\n",
            "Epoch 29, Loss: 0.0056011737371924695\n",
            " Loss: 0.06619663370661415\n",
            "Epoch 30, Loss: 0.0054086211404713765\n",
            " Loss: 0.06659588588002749\n",
            "Epoch 31, Loss: 0.005259571054556679\n",
            " Loss: 0.06662221811065294\n",
            "Epoch 32, Loss: 0.005077713106637016\n",
            " Loss: 0.06815994526843763\n",
            "Epoch 33, Loss: 0.004980718613362564\n",
            " Loss: 0.06703018781620228\n",
            "Epoch 34, Loss: 0.004812196210496001\n",
            " Loss: 0.06680081942655179\n",
            "Epoch 35, Loss: 0.004638609167198086\n",
            " Loss: 0.06642402426597496\n",
            "Epoch 36, Loss: 0.004544542727819177\n",
            " Loss: 0.0691485075763852\n",
            "Epoch 37, Loss: 0.0043746910309533215\n",
            " Loss: 0.06703092111462292\n",
            "Epoch 38, Loss: 0.004228786549167032\n",
            " Loss: 0.06681232261803272\n",
            "Epoch 39, Loss: 0.004098469381070962\n",
            " Loss: 0.06923070388939158\n",
            "Epoch 40, Loss: 0.003993155631752534\n",
            " Loss: 0.0672964304728924\n",
            "Epoch 41, Loss: 0.00389555364834524\n",
            " Loss: 0.06757728169977317\n",
            "Epoch 42, Loss: 0.003796179986060304\n",
            " Loss: 0.06730979821403922\n",
            "Epoch 43, Loss: 0.003692761743016767\n",
            " Loss: 0.0674914258618796\n",
            "Epoch 44, Loss: 0.003591505569984331\n",
            " Loss: 0.06871125577073972\n",
            "Epoch 45, Loss: 0.0034988781580066524\n",
            " Loss: 0.06807151697528312\n",
            "Epoch 46, Loss: 0.0034321139233271483\n",
            " Loss: 0.06836943794829714\n",
            "Epoch 47, Loss: 0.0033232766812621825\n",
            " Loss: 0.06920771935001307\n",
            "Epoch 48, Loss: 0.0032618458162086443\n",
            " Loss: 0.06817464109353402\n",
            "Epoch 49, Loss: 0.0031576227818514956\n",
            " Loss: 0.0685720407790813\n",
            "Epoch 50, Loss: 0.0030865208630710453\n",
            " Loss: 0.06841952626567784\n",
            "Training is finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Implementing Early Stopping**"
      ],
      "metadata": {
        "id": "bLRjP-dEA1M1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Complete this code to implement Early stopping\n",
        "patience = 5\n",
        "min_delta = 0.01\n",
        "best_loss = None\n",
        "patience_counter = 0\n",
        "\n",
        "# Training loop with early stopping\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        # Training pass\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # evaluation phase\n",
        "    model.eval()\n",
        "    validation_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            output = model(images)\n",
        "            validation_loss += loss.item()\n",
        "\n",
        "    # Calculate average losses\n",
        "    training_loss = running_loss / len(train_loader)\n",
        "    validation_loss /= len(test_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Training Loss: {training_loss}, Validation Loss: {validation_loss}\")\n",
        "\n",
        "    # Early stopping logic\n",
        "    if best_loss is None or validation_loss < best_loss - min_delta:\n",
        "        best_loss = validation_loss\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(\"Early stopping triggered!\")\n",
        "            break\n",
        "\n",
        "print(\"Training is finished!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3TryrSN5-U2",
        "outputId": "bf278e3f-041f-4453-cc04-c44f4ac46fe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training Loss: 0.0012245563557371497, Validation Loss: 0.0012245563557371497\n",
            "Epoch 2, Training Loss: 0.0012245563557371497, Validation Loss: 0.0012245563557371497\n",
            "Epoch 3, Training Loss: 0.0012245563557371497, Validation Loss: 0.0012245563557371497\n",
            "Epoch 4, Training Loss: 0.0012245563557371497, Validation Loss: 0.0012245563557371497\n",
            "Epoch 5, Training Loss: 0.0012245563557371497, Validation Loss: 0.0012245563557371497\n",
            "Epoch 6, Training Loss: 0.0012245563557371497, Validation Loss: 0.0012245563557371497\n",
            "Early stopping triggered!\n",
            "Training is finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experimenting with Dropout"
      ],
      "metadata": {
        "id": "VyEoEIZwBTK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NetWithDropout(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NetWithDropout, self).__init__()\n",
        "        # Define layers of the neural network\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)\n",
        "        self.dropout1 = nn.Dropout()  # Dropout layer with 20% probability\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.dropout2 = nn.Dropout()  # Dropout layer with 50% probability\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Flatten the input tensor\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        # Forward pass with dropout\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Create an instance of the network with dropout\n",
        "model_with_dropout = NetWithDropout()\n",
        "print(model_with_dropout)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aysL2O3QBUMq",
        "outputId": "ce755e3f-28a7-49f6-aa45-89ca2126556a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NetWithDropout(\n",
            "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
            "  (dropout1): Dropout(p=0.5, inplace=False)\n",
            "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (dropout2): Dropout(p=0.5, inplace=False)\n",
            "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Train the dropout model\n",
        "# What do you notice ?"
      ],
      "metadata": {
        "id": "TBNuaAo2BbB5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}